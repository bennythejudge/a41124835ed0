{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 08: semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a function to calculate a *MinHash*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasketch import MinHash\n",
    "\n",
    "def mh_digest (data):\n",
    "    mh = MinHash(num_perm=512)\n",
    "\n",
    "    for d in data:\n",
    "        mh.update(d.encode('utf8'))\n",
    "\n",
    "    return mh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll iterate through each parsed document, adding the keywords to the MinHash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a4.json {'editorial', 'advice', 'pass', 'step', 'during', 'plan', 'co', 'break', 'location', 'involve', 'physical', 'realize', 'way', 'progress', 'group', 'situation', 'sum', 'exam', 'student', 'guidance', 'online', 'interact', 'everyone', 'friend', 'author', 'classroom', 'conference', 'learning', 'reader', 'early', 'year', 'feedback', 'long', 'whether', '2016', 'loop', 'look', 'shoulder', 'company', 'ongoing', 'instructor', 'person', 'past', 'book', 'course', 'begin', 'standard', 'team', 'bring', 'hang', 'wise', 'training', 'back', 'worker', 'seasoned', 'experienced', '50%', 'always', 'people', 'live', 'attendance', 'introduce', 'expert', 'participate', 'knowledge', \"o'reilly\", 'media', 'process', 'attend', 'addition', 'learn', 'virtual', 'particular', 'analyze', \"mid-'80s\", 'individual', 'assess', 'shot', 'instructional', 'together', 'publish', 'video', 'provide', 'apply', 'start', 'experience'}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'a3.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5c5c133f434e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpynlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neo/Desktop/workspace/get_started_with_NLP_in_Python/a41124835ed0/pynlp.py\u001b[0m in \u001b[0;36mlex_iter\u001b[0;34m(json_file)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0miterate\u001b[0m \u001b[0mthrough\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mJSON\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'a3.json'"
     ]
    }
   ],
   "source": [
    "import pynlp\n",
    "\n",
    "files = [\"a4.json\", \"a3.json\", \"a2.json\", \"a1.json\"]\n",
    "\n",
    "stopwords = pynlp.load_stopwords(\"stop.txt\")\n",
    "files_set = {}\n",
    "files_mh = {}\n",
    "\n",
    "for json_file in files:\n",
    "    keywords = set([])\n",
    "\n",
    "    for lex in pynlp.lex_iter(json_file):\n",
    "        if (lex.pos != \".\") and (lex.root not in stopwords):\n",
    "            keywords.add(lex.root)\n",
    "\n",
    "    files_set[json_file] = keywords\n",
    "    files_mh[json_file] = mh_digest(keywords)\n",
    "\n",
    "    print(json_file, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the HTML documents, using a pairwise MinHash to approximate their Jaccard similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "sim = []\n",
    "\n",
    "for i1, i2 in itertools.combinations(range(len(files)), 2):\n",
    "    j = files_mh[files[i1]].jaccard(files_mh[files[i2]])\n",
    "    sim.append((j, files[i1], files[i2],))\n",
    "\n",
    "for jaccard, file1, file2 in sorted(sim, key=lambda x: x[0], reverse=True):\n",
    "    print(\"%0.4f\\t%s\\t%s\" % (jaccard, file1, file2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the top-ranked (\"most similar\") pair, where both `html/article2.html` and `html/article3.html` are about machine learning. Take a look at their overlapping keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_set[\"a3.json\"] & files_set[\"a2.json\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
